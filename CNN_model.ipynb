{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_906361904.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLDvVgXK2yCw"
      },
      "source": [
        "#Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "#CNN Imports\n",
        "import os\n",
        "import sys\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Activation, Input, Embedding, Reshape, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
        "from keras.layers import MaxPooling1D, GlobalMaxPooling1D \n",
        "from keras.layers import MaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxEtS-ny2-MU",
        "outputId": "77199710-4588-4986-f527-cd1a327d5a09"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBtcbcts2-Q6"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "prefix = \"/content/gdrive/My Drive/NLP Assignments/\"\n",
        "sys.path.append(prefix)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VA6V8NY2-US"
      },
      "source": [
        "#Reading Train, validation & Test dataset\n",
        "train_filename = prefix+\"Train.csv\"\n",
        "train_reviews =pd.read_csv(train_filename)\n",
        "\n",
        "valid_filename = prefix+\"Valid.csv\"\n",
        "valid_reviews =pd.read_csv(valid_filename)\n",
        "\n",
        "test_filename = prefix+\"Test.csv\"\n",
        "test_reviews =pd.read_csv(test_filename)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpwD814u2-YX",
        "outputId": "3867235f-936e-4db5-ce20-58cc6d88ce07"
      },
      "source": [
        "# Data preprocessing\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "    tokens = re.split('\\W+', text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(reviews['clean_text2'],\n",
        "                                                    #reviews['label'], test_size=0.2, random_state = 1000)\n",
        "train_reviews['clean_text2'] = train_reviews['text'].apply(lambda x: clean_text(x))\n",
        "valid_reviews['clean_text2'] = valid_reviews['text'].apply(lambda x: clean_text(x))\n",
        "test_reviews['clean_text2'] = test_reviews['text'].apply(lambda x: clean_text(x))\n",
        "\n",
        "X_train, y_train= train_reviews['clean_text2'], train_reviews['label']\n",
        "X_valid, y_valid= valid_reviews['clean_text2'], valid_reviews['label']\n",
        "X_test, y_test = test_reviews['clean_text2'], test_reviews['label']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gk4cT7Z2-bH",
        "outputId": "60528935-b91c-4d38-9a16-469458644616"
      },
      "source": [
        "#Feature Engineering to decide max words & padding sequence length\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "df = pd.concat([train_reviews, valid_reviews])\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def count_sent(text):\n",
        "    return len(nltk.sent_tokenize(text)) \n",
        "\n",
        "df['word_count'] = df[\"text\"].apply(lambda x:count_words(x))\n",
        "df['sent_count'] = df[\"text\"].apply(lambda x:count_sent(x))\n",
        "df['avg_sentlength'] = df['word_count']/df['sent_count']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IQapbcsA4Bg"
      },
      "source": [
        "#Setting hyperparameters\n",
        "MAX_WORDS = 2500\n",
        "MAX_SEQUENCE_LENGTH = 734\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "embedding_dim = 100\n",
        "random_state = 1000\n",
        "drop = 0.5\n",
        "batch_size = 32\n",
        "epochs = 5"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EACl9gjUA4Eu",
        "outputId": "7097e757-5ea1-4005-a10b-bba01b03bd7a"
      },
      "source": [
        "#Tokenizing\n",
        "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# saving tokenizer in pickle file which can be used later during testing\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    \n",
        "X_train_seq =  tokenizer.texts_to_sequences(X_train)\n",
        "X_valid_seq =  tokenizer.texts_to_sequences(X_valid)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"unique words : {}\".format(len(word_index)))\n",
        "\n",
        "X_train_data = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_valid_data = pad_sequences(X_valid_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test_data = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels_train = to_categorical(np.asarray(y_train))\n",
        "labels_valid = to_categorical(np.asarray(y_valid))\n",
        "labels_test = to_categorical(np.asarray(y_test))\n",
        "\n",
        "# Shapes of traing, valid and test tensors\n",
        "print('Shape of training data tensor:', X_train_data.shape)\n",
        "print('Shape of validating data tensor:', X_valid_data.shape)\n",
        "print('Shape of testing data tensor:', X_test_data.shape)\n",
        "\n",
        "print('Shape of training label tensor:', labels_train.shape)\n",
        "print('Shape of validating label tensor:', labels_valid.shape)\n",
        "print('Shape of testing label tensor:', labels_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words : 124271\n",
            "Shape of training data tensor: (40000, 734)\n",
            "Shape of validating data tensor: (5000, 734)\n",
            "Shape of testing data tensor: (5000, 734)\n",
            "Shape of training label tensor: (40000, 2)\n",
            "Shape of validating label tensor: (5000, 2)\n",
            "Shape of testing label tensor: (5000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAyXZY70A4JP",
        "outputId": "da39e338-f04d-4b57-be56-d7262ad29bb5"
      },
      "source": [
        "#Using Pre-Trained GLOVE Embeddings https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "embeddings_index = {}\n",
        "Glove_filename = prefix+\"glove.6B.100d.txt\"\n",
        "\n",
        "f = open(os.path.join(Glove_filename), encoding = \"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZccoslHA4NQ"
      },
      "source": [
        "#Creating embedding matrix & embedding layer\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector # words not found in embedding index will be all-zeros.\n",
        "\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T236ZFMDA4QB",
        "outputId": "a22bcdfb-396b-4146-e3c9-523888a44a42"
      },
      "source": [
        "#CNN layers\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "l_cov1= Conv1D(128, 1, activation='relu' ,padding='valid', kernel_initializer='normal')(embedded_sequences) \n",
        "l_cov2 = Conv1D(128, 2, activation='relu' ,padding='valid', kernel_initializer='normal')(l_cov1) # (l_pool1)\n",
        "g_pool = GlobalMaxPooling1D()(l_cov2)\n",
        "dropout = Dropout(0.5)(g_pool)\n",
        "preds = Dense(1, activation='sigmoid')(dropout)\n",
        "\n",
        "# this creates a model\n",
        "model = Model(inputs=sequence_input, outputs=preds)\n",
        "checkpoint = ModelCheckpoint('weights_cnn_sentence.hdf5', monitor='val_accuracy', verbose=1, save_best_only= True, mode='auto')\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 734)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 734, 100)          12427200  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 734, 128)          12928     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 733, 128)          32896     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 12,473,153\n",
            "Trainable params: 12,473,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjcIKeZq2-eT",
        "outputId": "04067fbe-e6b8-40a0-a1b3-29a5faa3adfb"
      },
      "source": [
        "#Traing CNN model\n",
        "print(\"Training Model...\")\n",
        "model.fit(X_train_data, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_valid_data, y_valid))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n",
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 65s 26ms/step - loss: 0.4366 - accuracy: 0.7917 - val_loss: 0.3311 - val_accuracy: 0.8598\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.85980, saving model to weights_cnn_sentence.hdf5\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.3280 - accuracy: 0.8576 - val_loss: 0.3014 - val_accuracy: 0.8730\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.85980 to 0.87300, saving model to weights_cnn_sentence.hdf5\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.2979 - accuracy: 0.8731 - val_loss: 0.2939 - val_accuracy: 0.8698\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.87300\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.2770 - accuracy: 0.8861 - val_loss: 0.2813 - val_accuracy: 0.8812\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.87300 to 0.88120, saving model to weights_cnn_sentence.hdf5\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 0.2628 - accuracy: 0.8895 - val_loss: 0.2847 - val_accuracy: 0.8810\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4afc47890>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMgQy-4lBNq3",
        "outputId": "938c36bc-a8dc-4b22-ea4e-8e1f04db8f59"
      },
      "source": [
        "#Saving model to json\n",
        "!pip install simplejson\n",
        "import simplejson\n",
        "import json\n",
        "model_json = model.to_json()\n",
        "json_filename = prefix+\"bestmodel.json\"\n",
        "with open(json_filename, \"w\") as json_file:\n",
        "    json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
        "json_file.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.7/dist-packages (3.17.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8agNb6gHBNuU",
        "outputId": "a206655a-014e-474a-895f-b43dc457741a"
      },
      "source": [
        "#Saving weights in h5 file\n",
        "model.save_weights(\"bestmodel.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuAG7zPRBNwU"
      },
      "source": [
        "json_path = prefix + \"bestmodel.json\"\n",
        "json_file = open(json_path, \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVkrhS7VBWHj",
        "outputId": "d2ed961c-3dce-4c55-8e29-548028f1b769"
      },
      "source": [
        "#Loading saved model for testing\n",
        "h5_filepath = prefix + \"bestmodel.h5\"\n",
        "loaded_model.load_weights(h5_filepath)\n",
        "print(\"Loaded model from disk\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CopM0NiBWKk",
        "outputId": "c3340719-cd86-4a42-e019-842fe4dc42a1"
      },
      "source": [
        "#Calculated accuracy on loaded model\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test_data, y_test, verbose=0)\n",
        "print (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 87.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whkPVnDiBWPS"
      },
      "source": [
        "#Calculating evalation parameters\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "# fit the model\n",
        "history = model.fit(X_train_data, y_train, validation_split=0.3, epochs=1, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss_train, accuracy_train, f1_score_train, precision_train, recall_train = model.evaluate(X_train_data, y_train, verbose=0)\n",
        "loss_valid, accuracy_valid, f1_score_valid, precision_valid, recall_valid = model.evaluate(X_valid_data, y_valid, verbose=0)\n",
        "loss_test, accuracy_test, f1_score_test, precision_test, recall_test = model.evaluate(X_test_data, y_test, verbose=0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9iEohFoBNy0",
        "outputId": "498ad153-381f-4079-eb23-7afe995c2fd0"
      },
      "source": [
        "print('F1-score on training data',f1_score_train)\n",
        "print('F1-score on validation data',f1_score_valid)\n",
        "print('F1-score on test data',f1_score_test)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score on training data 0.9272326231002808\n",
            "F1-score on validation data 0.877922773361206\n",
            "F1-score on test data 0.8698582053184509\n"
          ]
        }
      ]
    }
  ]
}